<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://boyeguillaume.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://boyeguillaume.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-04T16:25:38+00:00</updated><id>https://boyeguillaume.github.io/feed.xml</id><title type="html">G. Boyé</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Improving Safety in Large Language Models</title><link href="https://boyeguillaume.github.io/blog/2025/llm-safety/" rel="alternate" type="text/html" title="Improving Safety in Large Language Models"/><published>2025-09-03T16:02:00+00:00</published><updated>2025-09-03T16:02:00+00:00</updated><id>https://boyeguillaume.github.io/blog/2025/llm-safety</id><content type="html" xml:base="https://boyeguillaume.github.io/blog/2025/llm-safety/"><![CDATA[<p>Following the recent release of <a href="https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509">Apertus-8B</a> and <a href="https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509">Apertus-70B</a> we (<em>at meditron</em>) tried to perform a finetuning of this models using our own medical dataset. We managed to significantly improve the performance of the model on medical tasks (though still very far from the SOTA).</p> <p>However we soon realized that this finetuning broke the model’s safety, and very simple prompt could lead to dangerous outputs. This post is about the different approaches we took to improve safety in large language models.</p> <h2 id="generation-of-the-dataset">Generation of the dataset</h2> <p>We wanted to generate a dataset that we could use to finetune our model to avoid harmful and hateful content. We first <em>downloaded a dataset of problematic prompts</em> and use it to retrieve the response from our LLM.</p> <p>Here is the script we use to generate <em>prompts</em> from dataset over huggingface:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">json</span>

<span class="n">SOURCES</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">LLM-LAT/harmful-dataset</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">"</span><span class="s">onepaneai/harmful-prompts</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">),</span>
<span class="p">]</span>

<span class="c1"># Load the datasets and generate a jsonl file containing all prompts
</span><span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">prompts.jsonl</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">source</span><span class="p">,</span> <span class="n">split</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">SOURCES</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Processing {} - {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">split</span><span class="p">))</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">.</span><span class="nf">tqdm</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">entry</span><span class="p">[</span><span class="n">field</span><span class="p">]</span>

        <span class="c1"># Prevent duplicates
</span>        <span class="n">prompt_key</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">lower</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">prompt_key</span> <span class="ow">in</span> <span class="n">d</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">d</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">prompt_key</span><span class="p">)</span>

        <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">({</span><span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">})</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <p>With this dataset, we then feed it to our model to generate responses. Those were then filtered by a <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Llama-3-8B-Instruct</a> model. This filtering only flags responses to be harmful or hateful, and does not attempt any correction. We then use a method called <em>DPO</em> (<em>Direct Preference Optimization</em>) <a class="citation" href="#rafailov2024directpreferenceoptimizationlanguage">(Rafailov et al., 2024)</a> to train the model to avoid such response. This method make use of a <strong>prompt</strong>, a <strong>positive</strong> sample (that the model should answer) and a <strong>negative</strong> sample (that the model should avoid). The sample that were flagged dangerous become <strong>negative samples</strong>. A placeholder sentence is then used as a <strong>positive sample</strong>. Crutially we also used general level prompt to show examples of prompt the model <strong>should answer</strong>.</p> <p>Here is the script we use to generate <em>responses</em> from the model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">json</span>
<span class="kn">import</span> <span class="n">sys</span>
<span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># Load the tokenizer and model
</span><span class="n">MODEL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/path/to/model</span><span class="sh">"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">MODEL</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">MODEL</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># For each prompt
</span><span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">prompts.jsonl</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fo</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">answers-</span><span class="si">{</span><span class="n">offset</span><span class="si">}</span><span class="s">-</span><span class="si">{</span><span class="n">modulo</span><span class="si">}</span><span class="s">.jsonl</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">.</span><span class="nf">tqdm</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">f</span><span class="p">))):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="nf">loads</span><span class="p">(</span><span class="n">line</span><span class="p">)[</span><span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">]</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">apply_chat_template</span><span class="p">([</span>
        <span class="p">{</span>
            <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span>
        <span class="p">}</span>
    <span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">fo</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">answer</span><span class="sh">"</span><span class="p">:</span> <span class="n">answer</span>
    <span class="p">})</span> <span class="o">+</span> <span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>We then choosed an hardcoded sentence for negative samples, alongside a internal dataset for example of prompts the model <strong>should answer</strong>. We then launched a DPO training.</p>]]></content><author><name></name></author><category term="AI"/><category term="LLM"/><category term="LLM-safety"/><category term="DPO"/><category term="training"/><summary type="html"><![CDATA[This blogpost explores the challenges we face at meditron and how we can improve safety in large language models.]]></summary></entry><entry><title type="html">a post with math</title><link href="https://boyeguillaume.github.io/blog/2015/math/" rel="alternate" type="text/html" title="a post with math"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://boyeguillaume.github.io/blog/2015/math</id><content type="html" xml:base="https://boyeguillaume.github.io/blog/2015/math/"><![CDATA[<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2\] <p>You can also use <code class="language-plaintext highlighter-rouge">\begin{equation}...\end{equation}</code> instead of <code class="language-plaintext highlighter-rouge">$$</code> for display mode math. MathJax will automatically number equations:</p> <p>\begin{equation} \label{eq:cauchy-schwarz} \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) \end{equation}</p> <p>and by adding <code class="language-plaintext highlighter-rouge">\label{...}</code> inside the equation environment, we can now refer to the equation using <code class="language-plaintext highlighter-rouge">\eqref</code>.</p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[an example of a blog post with some math]]></summary></entry></feed>